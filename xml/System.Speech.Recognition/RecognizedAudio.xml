<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.2" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="RecognizedAudio.xml" source-language="en-US" target-language="de-DE">
    <header>
      <tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-15c36f0" tool-company="Microsoft" />
      <xliffext:skl_file_name xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">02cd5861-7ce2-4a82-b358-31f8435a0ac57ab9a89b28f9a235b4c2003b22ac60f8fa842abb.skl</xliffext:skl_file_name>
      <xliffext:version xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1.2</xliffext:version>
      <xliffext:ms.openlocfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">7ab9a89b28f9a235b4c2003b22ac60f8fa842abb</xliffext:ms.openlocfilehash>
      <xliffext:ms.sourcegitcommit xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">d31dc2ede16f6f7bc64e90d9f897ff54c4e3869b</xliffext:ms.sourcegitcommit>
      <xliffext:ms.lasthandoff xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">04/03/2018</xliffext:ms.lasthandoff>
      <xliffext:moniker_ids xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">netframework-4.5.1,netframework-4.5.2,netframework-4.5,netframework-4.6.1,netframework-4.6.2,netframework-4.6,netframework-4.7.1,netframework-4.7</xliffext:moniker_ids>
    </header>
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>Represents audio input that is associated with a <ph id="ph1">&lt;see cref="T:System.Speech.Recognition.RecognitionResult" /&gt;</ph>.</source>
          <target state="translated">Stellt eine Audioeingabe dar, die mit einem <ph id="ph1">&lt;see cref="T:System.Speech.Recognition.RecognitionResult" /&gt;</ph> zugeordnet ist.</target>       </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>A speech recognizer generates information about the audio input as part of the recognition operation.</source>
          <target state="translated">Eine von der Spracherkennung generiert Informationen zu den Audioeingabe als Teil der Erkennungsvorgang.</target>       </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>To access the recognized audio, use the <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;</ph> property or the <ph id="ph2">&lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt;</ph> method of the <ph id="ph3">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph>.</source>
          <target state="translated">Verwenden Sie den Zugriff auf den erkannten Audio der <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;</ph> Eigenschaft oder die <ph id="ph2">&lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt;</ph> Methode der <ph id="ph3">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph>.</target>       </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>A recognition result can be produced by the following events and methods of the <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> and <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> classes:</source>
          <target state="translated">Ein Erkennungsergebnis kann erstellt werden, durch die folgenden Ereignisse und Methoden des der <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> und <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> Klassen:</target>       </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>Events:</source>
          <target state="translated">Ereignisse:</target>       </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=nameWithType&gt;</ph> and <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=nameWithType&gt;</ph></source>
          <target state="translated"><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=nameWithType&gt;</ph> und <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=nameWithType&gt;</ph></target>       </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=nameWithType&gt;</ph> and <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=nameWithType&gt;</ph></source>
          <target state="translated"><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=nameWithType&gt;</ph> und <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=nameWithType&gt;</ph></target>       </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> and <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph></source>
          <target state="translated"><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> und <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph></target>       </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=nameWithType&gt;</ph> and <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=nameWithType&gt;</ph></source>
          <target state="translated"><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=nameWithType&gt;</ph> und <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=nameWithType&gt;</ph></target>       </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>Methods:</source>
          <target state="translated">Methoden:</target>       </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=nameWithType&gt;</ph> and <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=nameWithType&gt;</ph></source>
          <target state="translated"><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=nameWithType&gt;</ph> und <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=nameWithType&gt;</ph></target>       </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=nameWithType&gt;</ph> and <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=nameWithType&gt;</ph></source>
          <target state="translated"><ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=nameWithType&gt;</ph> und <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=nameWithType&gt;</ph></target>       </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>A recognition result produced by emulated speech recognition does not contain recognized audio.</source>
          <target state="translated">Eine von "emuliert" Spracherkennung erstellte Recognition Ergebnis enthält keine bekannten Audio.</target>       </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>For such a recognition result, its <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;</ph> property returns <ph id="ph2">`null`</ph> and its <ph id="ph3">&lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt;</ph> method throws an exception.</source>
          <target state="translated">Für solche Recognition Ergebnis seine <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;</ph> -Eigenschaft gibt <ph id="ph2">`null`</ph> und seine <ph id="ph3">&lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt;</ph> Methode löst eine Ausnahme aus.</target>       </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>For more information about emulated speech recognition, see the <ph id="ph1">`EmulateRecognize`</ph> and <ph id="ph2">`EmulateRecognizeAsync`</ph> methods of the <ph id="ph3">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> and <ph id="ph4">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> classes.</source>
          <target state="translated">Weitere Informationen zu "emuliert" Spracherkennung, finden Sie unter der <ph id="ph1">`EmulateRecognize`</ph> und <ph id="ph2">`EmulateRecognizeAsync`</ph> Methoden die <ph id="ph3">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> und <ph id="ph4">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> Klassen.</target>       </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve" extradata="MT" uid="T:System.Speech.Recognition.RecognizedAudio">
          <source>The following example handles the <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph>, <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph>, or <ph id="ph3">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=nameWithType&gt;</ph> event and outputs to the console information about the recognized audio that is associated with the recognition result.</source>
          <target state="translated">Das folgende Beispiel verarbeitet die <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph>, <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph>, oder <ph id="ph3">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=nameWithType&gt;</ph> Ereignis und Ausgaben an die Konsole Informationen über die erkannten Audiodatei, die das Erkennungsergebnis zugeordnet ist.</target>       </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve" uid="P:System.Speech.Recognition.RecognizedAudio.AudioPosition">
          <source>Gets the location in the input audio stream for the start of the recognized audio.</source>
          <target state="translated">Ruft die Position im Eingabeaudiostream für den Anfang des erkannten Audios ab.</target>       </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.AudioPosition">
          <source>The location in the input audio stream for the start of the recognized audio.</source>
          <target state="translated">Die Position im Eingabeaudiostream für den Anfang des erkannten Audios.</target>       </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.AudioPosition">
          <source>This property references the position at the beginning of the recognized phrase in the input device's generated audio stream.</source>
          <target state="translated">Diese Eigenschaft verweist auf die Position am Anfang des dem erkannten Ausdruck in das Eingabegerät generierten Audiostream.</target>       </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.AudioPosition">
          <source>By contrast, the <ph id="ph1">`RecognizerAudioPosition`</ph> property of the <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> and <ph id="ph3">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> classes reference the recognizer's position within its audio input.</source>
          <target state="translated">Im Gegensatz dazu, die <ph id="ph1">`RecognizerAudioPosition`</ph> Eigenschaft von der <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> und <ph id="ph3">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> Klassen verweisen, die Erkennung Position innerhalb der Audioeingabe.</target>       </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.AudioPosition">
          <source>These positions can be different.</source>
          <target state="translated">Diese Positionen können unterschiedlich sein.</target>       </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.AudioPosition">
          <source>For more information, see <bpt id="p1">[</bpt>Using Speech Recognition Events<ept id="p1">](http://msdn.microsoft.com/library/01c598ca-2e0e-4e89-b303-cd1cef9e8482)</ept>.</source>
          <target state="translated">Weitere Informationen finden Sie unter <bpt id="p1">[</bpt>mit Spracherkennung Recognition Ereignissen<ept id="p1">](http://msdn.microsoft.com/library/01c598ca-2e0e-4e89-b303-cd1cef9e8482)</ept>.</target>       </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.AudioPosition">
          <source>The <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt;</ph> property gets the system time at the start of the recognition operation.</source>
          <target state="translated">Die <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt;</ph> -Eigenschaft ruft die Systemzeit zu Beginn der Erkennungsvorgang ab.</target>       </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.AudioPosition">
          <source>The following example handles the <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> or <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph> event and outputs to the console information about the recognized audio that is associated with the recognition result.</source>
          <target state="translated">Das folgende Beispiel verarbeitet die <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> oder <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph> Ereignis und Ausgaben an die Konsole Informationen über die erkannten Audiodatei, die das Erkennungsergebnis zugeordnet ist.</target>       </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve" uid="P:System.Speech.Recognition.RecognizedAudio.Duration">
          <source>Gets the duration of the input audio stream for the recognized audio.</source>
          <target state="translated">Ruft die Dauer des Eingabeaudiostreams für das erkannte Audio ab.</target>       </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.Duration">
          <source>The duration within the input audio stream for the recognized audio.</source>
          <target state="translated">Die Dauer innerhalb des Eingabeaudiostreams für das erkannte Audio.</target>       </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.Duration">
          <source>The following example handles the <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> or <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph> event and outputs to the console information about the recognized audio that is associated with the recognition result.</source>
          <target state="translated">Das folgende Beispiel verarbeitet die <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> oder <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph> Ereignis und Ausgaben an die Konsole Informationen über die erkannten Audiodatei, die das Erkennungsergebnis zugeordnet ist.</target>       </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve" uid="P:System.Speech.Recognition.RecognizedAudio.Format">
          <source>Gets the format of the audio processed by a recognition engine.</source>
          <target state="translated">Ruft das Audioformat ab, das von einem Erkennungsmodul verarbeitet wird.</target>       </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.Format">
          <source>The format of the audio processed by the speech recognizer.</source>
          <target state="translated">Das Format für das Audio-Element, dass von der Spracherkennung verarbeitet wird.</target>       </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.Format">
          <source>The following example handles the <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> or <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph> event and outputs to the console information about the recognized audio that is associated with the recognition result.</source>
          <target state="translated">Das folgende Beispiel verarbeitet die <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> oder <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph> Ereignis und Ausgaben an die Konsole Informationen über die erkannten Audiodatei, die das Erkennungsergebnis zugeordnet ist.</target>       </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source>The starting point of the audio data to be returned.</source>
          <target state="translated">Der Anfangspunkt der Audiodaten, die zurückgegeben werden sollen.</target>       </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source>The length of the segment to be returned.</source>
          <target state="translated">Die Länge des Segments, das zurückgegeben werden soll.</target>       </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source>Selects and returns a section of the current recognized audio as binary data.</source>
          <target state="translated">Wählt aus und gibt einen Abschnitt des aktuellen erkannten Audio als Binärdaten zurück.</target>       </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source>Returns a subsection of the recognized audio, as defined by <ph id="ph1">&lt;paramref name="audioPosition" /&gt;</ph> and <ph id="ph2">&lt;paramref name="duration" /&gt;</ph>.</source>
          <target state="translated">Gibt einen Unterabschnitt des erkannten Audio zurück, wie durch <ph id="ph1">&lt;paramref name="audioPosition" /&gt;</ph> und <ph id="ph2">&lt;paramref name="duration" /&gt;</ph> definiert.</target>       </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source>The following example creates a speech recognition grammar for name input, adds a handler for the <ph id="ph1">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> event, and loads the grammar into an in-process speech recognizer.</source>
          <target state="translated">Das folgende Beispiel erstellt eine Spracherkennung Recognition Grammatik für die Namenseingabe, fügt ein Handler für das <ph id="ph1">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> -Ereignis und die Grammatik in einer in-Process-Spracherkennung geladen.</target>       </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source>Then it writes the audio information for the name portion of the input to an audio file.</source>
          <target state="translated">Er schreibt dann die Informationen für den Namensteil der Eingabe eine Audiodatei.</target>       </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source>The audio file is used as input to a <ph id="ph1">&lt;xref:System.Speech.Synthesis.SpeechSynthesizer&gt;</ph> object, which speaks a phrase that includes the recorded audio.</source>
          <target state="translated">Die Audiodatei dient als Eingabe für eine <ph id="ph1">&lt;xref:System.Speech.Synthesis.SpeechSynthesizer&gt;</ph> -Objekt, das einen Ausdruck spricht, die Audioaufnahmen enthält.</target>       </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source><ph id="ph1">&lt;paramref name="audioPosition" /&gt;</ph> and <ph id="ph2">&lt;paramref name="duration" /&gt;</ph> define a segment of audio outside the range of the current segment.</source>
          <target state="translated"><ph id="ph1">&lt;paramref name="audioPosition" /&gt;</ph> und <ph id="ph2">&lt;paramref name="duration" /&gt;</ph> definieren ein Audio-Segment außerhalb des Bereichs des aktuellen Segments.</target>       </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)">
          <source>The current recognized audio contains no data.</source>
          <target state="translated">Das aktuelle erkannte Audiodatei enthält keine Daten.</target>       </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve" uid="P:System.Speech.Recognition.RecognizedAudio.StartTime">
          <source>Gets the system time at the start of the recognition operation.</source>
          <target state="translated">Ruft die Systemzeit zu Beginn des Erkennungsvorgangs ab.</target>       </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.StartTime">
          <source>The system time at the start of the recognition operation.</source>
          <target state="translated">Die Systemzeit am Anfang des Erkennungsvorgangs.</target>       </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.StartTime">
          <source>The <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt;</ph> property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.</source>
          <target state="translated">Die <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt;</ph> Eigenschaft ruft die Systemzeit zu Beginn der Erkennungsvorgang, Latenz und Leistung von Berechnungen hilfreich sein kann.</target>       </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.StartTime">
          <source>The <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A&gt;</ph> property gets the location in the input device's generated audio stream.</source>
          <target state="translated">Die <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A&gt;</ph> Eigenschaft ruft den Speicherort in das Eingabegerät generierten Audiostream ab.</target>       </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve" extradata="MT" uid="P:System.Speech.Recognition.RecognizedAudio.StartTime">
          <source>The following example handles the <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> or <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph> event and outputs to the console information about the recognized audio that is associated with the recognition result.</source>
          <target state="translated">Das folgende Beispiel verarbeitet die <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType&gt;</ph> oder <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType&gt;</ph> Ereignis und Ausgaben an die Konsole Informationen über die erkannten Audiodatei, die das Erkennungsergebnis zugeordnet ist.</target>       </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)">
          <source>The stream that will receive the audio data.</source>
          <target state="translated">Der Stream, der die Audiodaten empfängt.</target>       </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)">
          <source>Writes the entire audio to a stream as raw data.</source>
          <target state="translated">Schreibt das gesamte Audio als Rohdaten in einen Stream.</target>       </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)">
          <source>Audio data is written to <ph id="ph1">`outputStream`</ph> in binary form.</source>
          <target state="translated">Audio Daten geschrieben <ph id="ph1">`outputStream`</ph> in binärer Form.</target>       </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)">
          <source>No header information is included.</source>
          <target state="translated">Keine Headerinformationen ist enthalten.</target>       </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)">
          <source>The <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt;</ph> method uses the Wave format, but does not include the Wave header.</source>
          <target state="translated">Die <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt;</ph> -Methode verwendet das Wave-Format, aber den Wave-Header nicht einschließt.</target>       </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)">
          <source>To include the Wave header, use the <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A&gt;</ph> method.</source>
          <target state="translated">Um den Wave-Header einzuschließen, verwenden die <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A&gt;</ph> Methode.</target>       </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)">
          <source>The stream that will receive the audio data.</source>
          <target state="translated">Der Stream, der die Audiodaten empfängt.</target>       </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)">
          <source>Writes audio to a stream in Wave format.</source>
          <target state="translated">Schreibt Audio in einen Stream im Waveformat.</target>       </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)">
          <source>Audio data is written to <ph id="ph1">`outputStream`</ph> in Wave format, which includes a resource interchange file format (RIFF) header.</source>
          <target state="translated">Audio Daten geschrieben <ph id="ph1">`outputStream`</ph> Wave-Format, darunter eine Ressource Interchange Format (RIFF) Dateiheader.</target>       </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)">
          <source>The <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt;</ph> method uses the same binary format, but does not include the Wave header.</source>
          <target state="translated">Die <ph id="ph1">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt;</ph> -Methode verwendet das gleiche binäre Format, aber den Wave-Header nicht einschließt.</target>       </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)">
          <source>The following example creates a speech recognition grammar for name input, adds a handler for the <ph id="ph1">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> event, and loads the grammar into an in-process speech recognizer.</source>
          <target state="translated">Das folgende Beispiel erstellt eine Spracherkennung Recognition Grammatik für die Namenseingabe, fügt ein Handler für das <ph id="ph1">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> -Ereignis und die Grammatik in einer in-Process-Spracherkennung geladen.</target>       </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)">
          <source>Then it writes the audio information for the name portion of the input to an audio file.</source>
          <target state="translated">Er schreibt dann die Informationen für den Namensteil der Eingabe eine Audiodatei.</target>       </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve" extradata="MT" uid="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)">
          <source>The audio file is used as input to a <ph id="ph1">&lt;xref:System.Speech.Synthesis.SpeechSynthesizer&gt;</ph> object, which speaks a phrase that includes the recorded audio.</source>
          <target state="translated">Die Audiodatei dient als Eingabe für eine <ph id="ph1">&lt;xref:System.Speech.Synthesis.SpeechSynthesizer&gt;</ph> -Objekt, das einen Ausdruck spricht, die Audioaufnahmen enthält.</target>       </trans-unit>
      </group>
    </body>
  </file>
</xliff>